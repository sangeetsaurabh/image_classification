{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Example of Image 7:\n",
      "Image - Min Value: 20 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 3 Name: cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDwCNCXoAaEzQA0Bjgh4AGhP0ANDY/wVHLp+Km5w66AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae6e9b9780>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 7\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batches.meta', 'data_batch_4', 'data_batch_1', 'readme.html', 'test_batch', 'data_batch_3', 'data_batch_5', 'data_batch_2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dirs = os.listdir(cifar10_dataset_folder_path)\n",
    "print (dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    num_labels = 10\n",
    "    labels = np.array(x)\n",
    "    # TODO: Implement Function\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "\n",
      "Batch is:  dict_keys(['data', 'batch_label', 'labels', 'filenames']):\n",
      "\n",
      "shape of Batch is: (10000, 3072):\n",
      "\n",
      "Shape of feature is:  (10000, 32, 32, 3):\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))\n",
    "print (valid_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x = tf.placeholder(tf.float32, shape=(None, image_shape[0], image_shape[1], image_shape[2]), name = 'x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes),name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=None,name = 'keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.shape[3].value, conv_num_outputs], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    conv = tf.nn.conv2d(x_tensor, layer1_weights, [1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    #conv = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.sigmoid(conv)\n",
    "    conv = tf.nn.max_pool(value=conv,ksize=[1, pool_ksize[0], pool_ksize[1], 1],strides=[1, pool_strides[0], pool_strides[1], 1],padding='SAME')\n",
    "    return conv \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value*x_tensor.shape[2].value*x_tensor.shape[3].value])\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    fc_weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.1))\n",
    "    fc_biases = tf.Variable(tf.constant(1.0, shape=[num_outputs]))\n",
    "    return tf.nn.sigmoid(tf.matmul(x_tensor, fc_weights) + fc_biases)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    out_weights = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.1))\n",
    "    out_biases = tf.Variable(tf.constant(1.0, shape=[num_outputs]))\n",
    "    return (tf.matmul(x_tensor, out_weights) + out_biases)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    test_num_outputs = 16\n",
    "    test_con_k = (4, 4)\n",
    "    test_con_s = (1, 1)\n",
    "    test_pool_k = (2, 2)\n",
    "    test_pool_s = (2, 2)\n",
    "    conv2d_maxpool_out = conv2d_maxpool(x, test_num_outputs, test_con_k, test_con_s, test_pool_k, test_pool_s)\n",
    "    conv2d_maxpool_out = conv2d_maxpool(conv2d_maxpool_out, test_num_outputs, test_con_k, test_con_s, test_pool_k, test_pool_s)\n",
    "    #print (conv2d_maxpool_out.shape)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_out = flatten(conv2d_maxpool_out)\n",
    "    #print (flat_out.shape)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    test_num_outputs = 64\n",
    "\n",
    "    fc_out = fully_conn(flat_out, test_num_outputs)\n",
    "    #fc_out = tf.nn.relu(fc_out)\n",
    "    #print (fc_out.shape)\n",
    "    # Apply Dropout\n",
    "    fc_out = tf.nn.dropout(fc_out, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    test_num_outputs = 10\n",
    "\n",
    "    output_out = output(fc_out, test_num_outputs)\n",
    "    \n",
    "    #print (output_out.shape)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    feed_dict = {x : feature_batch, y : label_batch, keep_prob : keep_probability}\n",
    "    session.run([optimizer], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    feed_dict = {x : feature_batch, y : label_batch, keep_prob : 1.0}\n",
    "    _,t_cost,t_accuracy = session.run([optimizer,cost,accuracy], feed_dict=feed_dict)\n",
    "    print('\\nTraining Cost - {}, Training Accuracy - {}  '.format(t_cost, t_accuracy), end='')\n",
    "    \n",
    "    feed_dict = {x : valid_features, y : valid_labels, keep_prob : 1.0}\n",
    "    v_accuracy = session.run(accuracy, feed_dict=feed_dict)\n",
    "    print('\\nValidation Accuracy - {}  '.format(v_accuracy), end='')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.300090789794922, Training Accuracy - 0.10000000149011612  \n",
      "Validation Accuracy - 0.10559999942779541  \n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.181236743927002, Training Accuracy - 0.17499999701976776  \n",
      "Validation Accuracy - 0.24219998717308044  \n",
      "Epoch  3, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.165139675140381, Training Accuracy - 0.17500001192092896  \n",
      "Validation Accuracy - 0.2572000026702881  \n",
      "Epoch  4, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.1533117294311523, Training Accuracy - 0.22500000894069672  \n",
      "Validation Accuracy - 0.2703999876976013  \n",
      "Epoch  5, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.143475294113159, Training Accuracy - 0.22500000894069672  \n",
      "Validation Accuracy - 0.2863999903202057  \n",
      "Epoch  6, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.1219699382781982, Training Accuracy - 0.2750000059604645  \n",
      "Validation Accuracy - 0.30319997668266296  \n",
      "Epoch  7, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.1017284393310547, Training Accuracy - 0.2750000059604645  \n",
      "Validation Accuracy - 0.33319997787475586  \n",
      "Epoch  8, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.061765670776367, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.3523999750614166  \n",
      "Epoch  9, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.045823097229004, Training Accuracy - 0.3999999761581421  \n",
      "Validation Accuracy - 0.3773999810218811  \n",
      "Epoch 10, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.01100492477417, Training Accuracy - 0.3999999761581421  \n",
      "Validation Accuracy - 0.38739997148513794  \n",
      "Epoch 11, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.940742015838623, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.3929999768733978  \n",
      "Epoch 12, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.9136631488800049, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.40699997544288635  \n",
      "Epoch 13, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.8772917985916138, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.4147999882698059  \n",
      "Epoch 14, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.845965027809143, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.41760000586509705  \n",
      "Epoch 15, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.7820008993148804, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.4195999503135681  \n",
      "Epoch 16, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.7834532260894775, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.4309999644756317  \n",
      "Epoch 17, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.765159010887146, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.43619996309280396  \n",
      "Epoch 18, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.701338291168213, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.44019997119903564  \n",
      "Epoch 19, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.673269271850586, Training Accuracy - 0.4000000059604645  \n",
      "Validation Accuracy - 0.4399999678134918  \n",
      "Epoch 20, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.653175950050354, Training Accuracy - 0.42500001192092896  \n",
      "Validation Accuracy - 0.44599997997283936  \n",
      "Epoch 21, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.6217368841171265, Training Accuracy - 0.42500001192092896  \n",
      "Validation Accuracy - 0.4503999650478363  \n",
      "Epoch 22, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.6022944450378418, Training Accuracy - 0.45000001788139343  \n",
      "Validation Accuracy - 0.4565999507904053  \n",
      "Epoch 23, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.5489497184753418, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.45819997787475586  \n",
      "Epoch 24, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.5413849353790283, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.46799999475479126  \n",
      "Epoch 25, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.4981834888458252, Training Accuracy - 0.45000001788139343  \n",
      "Validation Accuracy - 0.4697999656200409  \n",
      "Epoch 26, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.4673420190811157, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.4729999601840973  \n",
      "Epoch 27, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.4335192441940308, Training Accuracy - 0.4750000238418579  \n",
      "Validation Accuracy - 0.4769999384880066  \n",
      "Epoch 28, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3931467533111572, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.48159998655319214  \n",
      "Epoch 29, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3700634241104126, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.48559996485710144  \n",
      "Epoch 30, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3458447456359863, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.49219995737075806  \n",
      "Epoch 31, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3168282508850098, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.49539995193481445  \n",
      "Epoch 32, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3103578090667725, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.49299994111061096  \n",
      "Epoch 33, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2823832035064697, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.4987999200820923  \n",
      "Epoch 34, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2588310241699219, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5019999742507935  \n",
      "Epoch 35, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2051180601119995, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5072000026702881  \n",
      "Epoch 36, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2066863775253296, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5041999816894531  \n",
      "Epoch 37, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1896162033081055, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5095999240875244  \n",
      "Epoch 38, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.176646113395691, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5072000026702881  \n",
      "Epoch 39, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1793962717056274, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5037999153137207  \n",
      "Epoch 40, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1532121896743774, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5125999450683594  \n",
      "Epoch 41, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1105108261108398, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5123999714851379  \n",
      "Epoch 42, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1176095008850098, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5137999653816223  \n",
      "Epoch 43, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.0902080535888672, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5129998922348022  \n",
      "Epoch 44, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.051391839981079, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5183998942375183  \n",
      "Epoch 45, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.03792142868042, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5143999457359314  \n",
      "Epoch 46, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9988477230072021, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5157999396324158  \n",
      "Epoch 47, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9863141775131226, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5173999667167664  \n",
      "Epoch 48, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9885261058807373, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5183999538421631  \n",
      "Epoch 49, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9742988348007202, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5179999470710754  \n",
      "Epoch 50, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9332634210586548, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5209999084472656  \n",
      "Epoch 51, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9278551340103149, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5199999213218689  \n",
      "Epoch 52, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9102320671081543, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5199999213218689  \n",
      "Epoch 53, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8744120597839355, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5241999626159668  \n",
      "Epoch 54, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8540079593658447, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5269999504089355  \n",
      "Epoch 55, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8625997304916382, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5261999368667603  \n",
      "Epoch 56, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8211792707443237, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5269998908042908  \n",
      "Epoch 57, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8276236653327942, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5235999822616577  \n",
      "Epoch 58, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7978341579437256, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5259999632835388  \n",
      "Epoch 59, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7926669120788574, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5255999565124512  \n",
      "Epoch 60, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7569670677185059, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5257999300956726  \n",
      "Epoch 61, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7370527982711792, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5295999646186829  \n",
      "Epoch 62, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7390624284744263, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5303999185562134  \n",
      "Epoch 63, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7257946729660034, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.5295999646186829  \n",
      "Epoch 64, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7157984375953674, Training Accuracy - 0.7500000596046448  \n",
      "Validation Accuracy - 0.5333999395370483  \n",
      "Epoch 65, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6840115785598755, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.535599946975708  \n",
      "Epoch 66, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.676065981388092, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.5303999185562134  \n",
      "Epoch 67, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6726564168930054, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.5329999327659607  \n",
      "Epoch 68, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6498056650161743, Training Accuracy - 0.824999988079071  \n",
      "Validation Accuracy - 0.5313999056816101  \n",
      "Epoch 69, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6397631168365479, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.5343999266624451  \n",
      "Epoch 70, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6141479015350342, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.5247999429702759  \n",
      "Epoch 71, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6046765446662903, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.530799925327301  \n",
      "Epoch 72, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6198822855949402, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.5299999117851257  \n",
      "Epoch 73, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5770792961120605, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.5303999781608582  \n",
      "Epoch 74, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5542441010475159, Training Accuracy - 0.8500000834465027  \n",
      "Validation Accuracy - 0.5273999571800232  \n",
      "Epoch 75, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5433288812637329, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.5305999517440796  \n",
      "Epoch 76, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5472850799560547, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.5303999781608582  \n",
      "Epoch 77, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5232824683189392, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5331999659538269  \n",
      "Epoch 78, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5080291032791138, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5297999382019043  \n",
      "Epoch 79, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4949183464050293, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5303999185562134  \n",
      "Epoch 80, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.488556444644928, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5341999530792236  \n",
      "Epoch 81, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.49038729071617126, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5261999368667603  \n",
      "Epoch 82, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.483513206243515, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5291999578475952  \n",
      "Epoch 83, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.47050976753234863, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5227999687194824  \n",
      "Epoch 84, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.45662829279899597, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.5246000289916992  \n",
      "Epoch 85, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4398851692676544, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.5269999504089355  \n",
      "Epoch 86, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4482195973396301, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5213999152183533  \n",
      "Epoch 87, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4625958800315857, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.5239999294281006  \n",
      "Epoch 88, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4228495955467224, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.5233999490737915  \n",
      "Epoch 89, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.423968642950058, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5257999300956726  \n",
      "Epoch 90, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3835955858230591, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5231999158859253  \n",
      "Epoch 91, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4221699833869934, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.525399923324585  \n",
      "Epoch 92, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.39545297622680664, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5257999300956726  \n",
      "Epoch 93, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.37441912293434143, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5279999375343323  \n",
      "Epoch 94, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3704276978969574, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5221999883651733  \n",
      "Epoch 95, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3621150851249695, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.524199903011322  \n",
      "Epoch 96, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3436720371246338, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5259999632835388  \n",
      "Epoch 97, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3455623984336853, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.52239990234375  \n",
      "Epoch 98, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3442045748233795, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5243999361991882  \n",
      "Epoch 99, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.33923229575157166, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.5273998975753784  \n",
      "Epoch 100, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3256329298019409, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5231999158859253  \n",
      "Epoch 101, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.32694828510284424, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5255999565124512  \n",
      "Epoch 102, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3196609914302826, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5285999178886414  \n",
      "Epoch 103, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3009851574897766, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5205999612808228  \n",
      "Epoch 104, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.309366375207901, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5229999423027039  \n",
      "Epoch 105, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3008623719215393, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5203999280929565  \n",
      "Epoch 106, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.29398584365844727, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5231999158859253  \n",
      "Epoch 107, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2904236316680908, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5217999815940857  \n",
      "Epoch 108, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.27462926506996155, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5227999091148376  \n",
      "Epoch 109, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.27637726068496704, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5235999226570129  \n",
      "Epoch 110, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2819969356060028, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5181999802589417  \n",
      "Epoch 111, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2646714448928833, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5291999578475952  \n",
      "Epoch 112, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2491849660873413, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5219999551773071  \n",
      "Epoch 113, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.24778509140014648, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5223999619483948  \n",
      "Epoch 114, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2505316138267517, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5217999219894409  \n",
      "Epoch 115, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2468748539686203, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5269998908042908  \n",
      "Epoch 116, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2391553670167923, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5231999158859253  \n",
      "Epoch 117, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.23690642416477203, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5183999538421631  \n",
      "Epoch 118, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2298676073551178, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5209999680519104  \n",
      "Epoch 119, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2338455468416214, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5195999145507812  \n",
      "Epoch 120, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2298268973827362, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5177999138832092  \n",
      "Epoch 121, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.23078203201293945, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5235999226570129  \n",
      "Epoch 122, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.22309798002243042, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5207999348640442  \n",
      "Epoch 123, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.21617229282855988, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5165999531745911  \n",
      "Epoch 124, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20773755013942719, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.518799901008606  \n",
      "Epoch 125, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2147665023803711, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5197999477386475  \n",
      "Epoch 126, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2074800431728363, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5171999931335449  \n",
      "Epoch 127, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20457202196121216, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5181999206542969  \n",
      "Epoch 128, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2067001461982727, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5161999464035034  \n",
      "Epoch 129, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1936338245868683, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5167999267578125  \n",
      "Epoch 130, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19581489264965057, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5125999450683594  \n",
      "Epoch 131, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19357870519161224, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5143999457359314  \n",
      "Epoch 132, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20067046582698822, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5107999444007874  \n",
      "Epoch 133, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19948521256446838, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5067999362945557  \n",
      "Epoch 134, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.18157818913459778, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5127999782562256  \n",
      "Epoch 135, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19950637221336365, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5087999105453491  \n",
      "Epoch 136, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1973710060119629, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5073999762535095  \n",
      "Epoch 137, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.17573700845241547, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5103999376296997  \n",
      "Epoch 138, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.15962664783000946, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5157999396324158  \n",
      "Epoch 139, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.16377916932106018, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5119999647140503  \n",
      "Epoch 140, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.15094701945781708, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5123999118804932  \n",
      "Epoch 141, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.17254126071929932, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.507599949836731  \n",
      "Epoch 142, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.15559618175029755, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5067999362945557  \n",
      "Epoch 143, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14856715500354767, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5147998929023743  \n",
      "Epoch 144, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.15532642602920532, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5169999599456787  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method TF_Buffer.<lambda> of <tensorflow.python.pywrap_tensorflow.TF_Buffer; proxy of <Swig Object of type 'TF_Buffer *' at 0x7fae6e9ce480> >>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 339, in <lambda>\n",
      "    __del__ = lambda self: None\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 145, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14921697974205017, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5117999315261841  \n",
      "Epoch 146, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1539030522108078, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5059999227523804  \n",
      "Epoch 147, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1460351198911667, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.507599949836731  \n",
      "Epoch 148, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14194734394550323, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5063999891281128  \n",
      "Epoch 149, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14546988904476166, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5135999917984009  \n",
      "Epoch 150, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13938187062740326, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5167999863624573  \n",
      "Epoch 151, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1352827101945877, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.514799952507019  \n",
      "Epoch 152, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13835442066192627, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5125999450683594  \n",
      "Epoch 153, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14196054637432098, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5107999444007874  \n",
      "Epoch 154, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13724385201931, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5175999402999878  \n",
      "Epoch 155, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13303865492343903, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.5155999660491943  \n",
      "Epoch 156, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1275349110364914, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5151998996734619  \n",
      "Epoch 157, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11686091870069504, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.509399950504303  \n",
      "Epoch 158, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11541223526000977, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5131999254226685  \n",
      "Epoch 159, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11739154160022736, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5151999592781067  \n",
      "Epoch 160, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12230326235294342, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.5143999457359314  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a54da232f64c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a22b7b32ade4>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# TODO: Implement Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('\\nEpoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.2845354080200195, Training Accuracy - 0.20000000298023224  \n",
      "Validation Accuracy - 0.16840000450611115  Epoch  1, CIFAR-10 Batch 2:  \n",
      "Training Cost - 2.141263961791992, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.24159999191761017  Epoch  1, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.9139361381530762, Training Accuracy - 0.32499998807907104  \n",
      "Validation Accuracy - 0.25839999318122864  Epoch  1, CIFAR-10 Batch 4:  \n",
      "Training Cost - 2.00004243850708, Training Accuracy - 0.17500001192092896  \n",
      "Validation Accuracy - 0.28540000319480896  Epoch  1, CIFAR-10 Batch 5:  \n",
      "Training Cost - 2.0716466903686523, Training Accuracy - 0.30000001192092896  \n",
      "Validation Accuracy - 0.29760000109672546  Epoch  2, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.125340700149536, Training Accuracy - 0.2750000059604645  \n",
      "Validation Accuracy - 0.3255999684333801  Epoch  2, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.9725000858306885, Training Accuracy - 0.32500001788139343  \n",
      "Validation Accuracy - 0.35420000553131104  Epoch  2, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.6111632585525513, Training Accuracy - 0.42500001192092896  \n",
      "Validation Accuracy - 0.36739999055862427  Epoch  2, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.745745062828064, Training Accuracy - 0.32500001788139343  \n",
      "Validation Accuracy - 0.3717999756336212  Epoch  2, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.8408703804016113, Training Accuracy - 0.25  \n",
      "Validation Accuracy - 0.3853999674320221  Epoch  3, CIFAR-10 Batch 1:  \n",
      "Training Cost - 2.017655372619629, Training Accuracy - 0.32500001788139343  \n",
      "Validation Accuracy - 0.4047999978065491  Epoch  3, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.7777173519134521, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.4023999571800232  Epoch  3, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.38992440700531, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.409199982881546  Epoch  3, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.5504114627838135, Training Accuracy - 0.44999998807907104  \n",
      "Validation Accuracy - 0.4177999496459961  Epoch  3, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.7195308208465576, Training Accuracy - 0.4000000059604645  \n",
      "Validation Accuracy - 0.42500001192092896  Epoch  4, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.8848387002944946, Training Accuracy - 0.32500001788139343  \n",
      "Validation Accuracy - 0.43619996309280396  Epoch  4, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.609222173690796, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.4371999502182007  Epoch  4, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.2814207077026367, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.43719998002052307  Epoch  4, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.4494273662567139, Training Accuracy - 0.4750000238418579  \n",
      "Validation Accuracy - 0.4429999589920044  Epoch  4, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.6414259672164917, Training Accuracy - 0.4750000238418579  \n",
      "Validation Accuracy - 0.4565999507904053  Epoch  5, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.7907259464263916, Training Accuracy - 0.375  \n",
      "Validation Accuracy - 0.45819997787475586  Epoch  5, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.5279908180236816, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.4615999758243561  Epoch  5, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.200406551361084, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.45459994673728943  Epoch  5, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.4265791177749634, Training Accuracy - 0.45000001788139343  \n",
      "Validation Accuracy - 0.45879998803138733  Epoch  5, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.5493214130401611, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.47179996967315674  Epoch  6, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.7110381126403809, Training Accuracy - 0.3999999761581421  \n",
      "Validation Accuracy - 0.4721999764442444  Epoch  6, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.4447308778762817, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.4779999554157257  Epoch  6, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.1447319984436035, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.47279998660087585  Epoch  6, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.3905717134475708, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.4811999797821045  Epoch  6, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.4726974964141846, Training Accuracy - 0.5750000476837158  \n",
      "Validation Accuracy - 0.4809999465942383  Epoch  7, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.6611251831054688, Training Accuracy - 0.3500000238418579  \n",
      "Validation Accuracy - 0.4877999722957611  Epoch  7, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.3597875833511353, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.4877999722957611  Epoch  7, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.086717128753662, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.488599956035614  Epoch  7, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.352738857269287, Training Accuracy - 0.4750000238418579  \n",
      "Validation Accuracy - 0.4939999580383301  Epoch  7, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.4668912887573242, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.49859997630119324  Epoch  8, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.6407907009124756, Training Accuracy - 0.44999998807907104  \n",
      "Validation Accuracy - 0.4979999363422394  Epoch  8, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.3361445665359497, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.5021999478340149  Epoch  8, CIFAR-10 Batch 3:  \n",
      "Training Cost - 1.0276402235031128, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.4971999526023865  Epoch  8, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.2776501178741455, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.5079998970031738  Epoch  8, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.4040223360061646, Training Accuracy - 0.5249999761581421  \n",
      "Validation Accuracy - 0.5072000026702881  Epoch  9, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.6454777717590332, Training Accuracy - 0.3999999761581421  \n",
      "Validation Accuracy - 0.5071999430656433  Epoch  9, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.306890606880188, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.5145999789237976  Epoch  9, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.982756495475769, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5101999640464783  Epoch  9, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.2400901317596436, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.5171999335289001  Epoch  9, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.3481770753860474, Training Accuracy - 0.5750000476837158  \n",
      "Validation Accuracy - 0.5165999531745911  Epoch 10, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.5719287395477295, Training Accuracy - 0.42500001192092896  \n",
      "Validation Accuracy - 0.5185999870300293  Epoch 10, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.2632505893707275, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5219999551773071  Epoch 10, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.958702802658081, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5271999835968018  Epoch 10, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.1981141567230225, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.5311999320983887  Epoch 10, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.2883901596069336, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5271999835968018  Epoch 11, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.534975528717041, Training Accuracy - 0.44999998807907104  \n",
      "Validation Accuracy - 0.5255999565124512  Epoch 11, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.2137389183044434, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5249999165534973  Epoch 11, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.9111837148666382, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.5285999178886414  Epoch 11, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.1385471820831299, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.5343999862670898  Epoch 11, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.2613158226013184, Training Accuracy - 0.5750000476837158  \n",
      "Validation Accuracy - 0.535599946975708  Epoch 12, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.5066044330596924, Training Accuracy - 0.45000001788139343  \n",
      "Validation Accuracy - 0.5357999205589294  Epoch 12, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.17462956905365, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5379999279975891  Epoch 12, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.8920668363571167, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5341999530792236  Epoch 12, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.1057156324386597, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5343999266624451  Epoch 12, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.1861109733581543, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5427999496459961  Epoch 13, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.4194837808609009, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.5463999509811401  Epoch 13, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.11917245388031, Training Accuracy - 0.5250000357627869  \n",
      "Validation Accuracy - 0.5453999042510986  Epoch 13, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.878504753112793, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5471999645233154  Epoch 13, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.1220829486846924, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5475998520851135  Epoch 13, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.159830927848816, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5461999177932739  Epoch 14, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.4050217866897583, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.5483999252319336  Epoch 14, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.089955449104309, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.5509999394416809  Epoch 14, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.8487659692764282, Training Accuracy - 0.675000011920929  \n",
      "Validation Accuracy - 0.5513999462127686  Epoch 14, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.0655443668365479, Training Accuracy - 0.6000000238418579  \n",
      "Validation Accuracy - 0.5573999285697937  Epoch 14, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.1257683038711548, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5541999340057373  Epoch 15, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3545008897781372, Training Accuracy - 0.5  \n",
      "Validation Accuracy - 0.5541999340057373  Epoch 15, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.0560791492462158, Training Accuracy - 0.5250000357627869  \n",
      "Validation Accuracy - 0.5603998899459839  Epoch 15, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.8290730714797974, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5557999610900879  Epoch 15, CIFAR-10 Batch 4:  \n",
      "Training Cost - 1.0408709049224854, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5607999563217163  Epoch 15, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.079582929611206, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5607999563217163  Epoch 16, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.3508577346801758, Training Accuracy - 0.5250000357627869  \n",
      "Validation Accuracy - 0.5597999095916748  Epoch 16, CIFAR-10 Batch 2:  \n",
      "Training Cost - 1.027586817741394, Training Accuracy - 0.4749999940395355  \n",
      "Validation Accuracy - 0.555199921131134  Epoch 16, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.7923680543899536, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5611999034881592  Epoch 16, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.9640291333198547, Training Accuracy - 0.675000011920929  \n",
      "Validation Accuracy - 0.5677999258041382  Epoch 16, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.03092360496521, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.5719999670982361  Epoch 17, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.322251319885254, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.5661998987197876  Epoch 17, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.9943312406539917, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5715999007225037  Epoch 17, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.7882834672927856, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5665999054908752  Epoch 17, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.9786139726638794, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5731999278068542  Epoch 17, CIFAR-10 Batch 5:  \n",
      "Training Cost - 1.0018997192382812, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5739999413490295  Epoch 18, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2745391130447388, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5687999129295349  Epoch 18, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.9367051720619202, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.5719999074935913  Epoch 18, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.7277941107749939, Training Accuracy - 0.6999999284744263  \n",
      "Validation Accuracy - 0.5727999210357666  Epoch 18, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.961432695388794, Training Accuracy - 0.675000011920929  \n",
      "Validation Accuracy - 0.5803998708724976  Epoch 18, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.9721850156784058, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5813999772071838  Epoch 19, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.2291879653930664, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.577799916267395  Epoch 19, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.9291642904281616, Training Accuracy - 0.574999988079071  \n",
      "Validation Accuracy - 0.5819998979568481  Epoch 19, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.7480229139328003, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5741998553276062  Epoch 19, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.934640645980835, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5819998979568481  Epoch 19, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.9539785385131836, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5813999176025391  Epoch 20, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1971793174743652, Training Accuracy - 0.550000011920929  \n",
      "Validation Accuracy - 0.577799916267395  Epoch 20, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.9116513729095459, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5849999189376831  Epoch 20, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.689268171787262, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.575999915599823  Epoch 20, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.8997662663459778, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.586199939250946  Epoch 20, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.9259942770004272, Training Accuracy - 0.675000011920929  \n",
      "Validation Accuracy - 0.5879998803138733  Epoch 21, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1546703577041626, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5831998586654663  Epoch 21, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.8519759178161621, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5913999676704407  Epoch 21, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6954888105392456, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5811998844146729  Epoch 21, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.8810666799545288, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5913999080657959  Epoch 21, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.9108255505561829, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5931999087333679  Epoch 22, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.1243176460266113, Training Accuracy - 0.625  \n",
      "Validation Accuracy - 0.5981999039649963  Epoch 22, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.8224122524261475, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.5961999297142029  Epoch 22, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6898167133331299, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.5827999114990234  Epoch 22, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.8281885385513306, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.5941998958587646  Epoch 22, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.8489272594451904, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5987998843193054  Epoch 23, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.0863763093948364, Training Accuracy - 0.6500000357627869  \n",
      "Validation Accuracy - 0.5937999486923218  Epoch 23, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.8005533814430237, Training Accuracy - 0.6499999761581421  \n",
      "Validation Accuracy - 0.6019999980926514  Epoch 23, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.655641496181488, Training Accuracy - 0.7500000596046448  \n",
      "Validation Accuracy - 0.5847999453544617  Epoch 23, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.8401837348937988, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.5987999439239502  Epoch 23, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.8364560604095459, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.5929999351501465  Epoch 24, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.0598784685134888, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.5953998565673828  Epoch 24, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.767444372177124, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.6007999181747437  Epoch 24, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6374516487121582, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.5899999737739563  Epoch 24, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7879108190536499, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.6025999784469604  Epoch 24, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.7768794894218445, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.602199912071228  Epoch 25, CIFAR-10 Batch 1:  \n",
      "Training Cost - 1.0453227758407593, Training Accuracy - 0.6500000357627869  \n",
      "Validation Accuracy - 0.5987999439239502  Epoch 25, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.7584670782089233, Training Accuracy - 0.6749999523162842  \n",
      "Validation Accuracy - 0.605199933052063  Epoch 25, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6140470504760742, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.5981999039649963  Epoch 25, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7747604846954346, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.6049998998641968  Epoch 25, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.7607543468475342, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.5979999303817749  Epoch 26, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9703950881958008, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.6061999201774597  Epoch 26, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.7342668771743774, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.6059999465942383  Epoch 26, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6052599549293518, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6017999053001404  Epoch 26, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7423362731933594, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.5991998910903931  Epoch 26, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.7163578271865845, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.6057998538017273  Epoch 27, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9779648780822754, Training Accuracy - 0.675000011920929  \n",
      "Validation Accuracy - 0.6071999073028564  Epoch 27, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.6895554065704346, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.6057999134063721  Epoch 27, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.6177113056182861, Training Accuracy - 0.7500000596046448  \n",
      "Validation Accuracy - 0.6031998991966248  Epoch 27, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7487707138061523, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.6133999228477478  Epoch 27, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.7154020667076111, Training Accuracy - 0.8000000715255737  \n",
      "Validation Accuracy - 0.6013998985290527  Epoch 28, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.9173738360404968, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.6101999282836914  Epoch 28, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.6685042381286621, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6119998693466187  Epoch 28, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5736908912658691, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.6057999730110168  Epoch 28, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7249906063079834, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6053999066352844  Epoch 28, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.6919706463813782, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6067999601364136  Epoch 29, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8957810997962952, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.6129999160766602  Epoch 29, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.686536431312561, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6137998700141907  Epoch 29, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5547794699668884, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.6075999140739441  Epoch 29, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.7251060605049133, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.6103999614715576  Epoch 29, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.643803060054779, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6125999093055725  Epoch 30, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.870684802532196, Training Accuracy - 0.7000000476837158  \n",
      "Validation Accuracy - 0.6109999418258667  Epoch 30, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.6162315011024475, Training Accuracy - 0.824999988079071  \n",
      "Validation Accuracy - 0.6165999174118042  Epoch 30, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5554179549217224, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.6097999215126038  Epoch 30, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6723566055297852, Training Accuracy - 0.7999999523162842  \n",
      "Validation Accuracy - 0.6091999411582947  Epoch 30, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.6497101783752441, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6123999357223511  Epoch 31, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8411583304405212, Training Accuracy - 0.7250000238418579  \n",
      "Validation Accuracy - 0.6151999235153198  Epoch 31, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.6409332156181335, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6193998456001282  Epoch 31, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5405372381210327, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6165999174118042  Epoch 31, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6533854007720947, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6203999519348145  Epoch 31, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.6207665801048279, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6105998754501343  Epoch 32, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.8452146053314209, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.6185999512672424  Epoch 32, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.6241750121116638, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.6185998916625977  Epoch 32, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5180943012237549, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6145999431610107  Epoch 32, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6565136313438416, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6185998916625977  Epoch 32, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.599888026714325, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.619399905204773  Epoch 33, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.821587085723877, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.6209998726844788  Epoch 33, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5779552459716797, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 33, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5141718983650208, Training Accuracy - 0.824999988079071  \n",
      "Validation Accuracy - 0.6135998964309692  Epoch 33, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6185156106948853, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6225998401641846  Epoch 33, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5995674133300781, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 34, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7828575372695923, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.6255999207496643  Epoch 34, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5759037137031555, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6239998936653137  Epoch 34, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5189248323440552, Training Accuracy - 0.7999999523162842  \n",
      "Validation Accuracy - 0.6225998997688293  Epoch 34, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6226725578308105, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.621199905872345  Epoch 34, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5754513144493103, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6195998787879944  Epoch 35, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7651935815811157, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.6301999092102051  Epoch 35, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5748410820960999, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6223998665809631  Epoch 35, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.5093352198600769, Training Accuracy - 0.7999999523162842  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 35, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.6022346615791321, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6231999397277832  Epoch 35, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5490157008171082, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6291999816894531  Epoch 36, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7581000924110413, Training Accuracy - 0.7749999761581421  \n",
      "Validation Accuracy - 0.6237999796867371  Epoch 36, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5630854964256287, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6233998537063599  Epoch 36, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.49252820014953613, Training Accuracy - 0.7999999523162842  \n",
      "Validation Accuracy - 0.6237999200820923  Epoch 36, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5981384515762329, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6275998950004578  Epoch 36, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5423433184623718, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 37, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7326147556304932, Training Accuracy - 0.75  \n",
      "Validation Accuracy - 0.6239998936653137  Epoch 37, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5504247546195984, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.625999927520752  Epoch 37, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.49732905626296997, Training Accuracy - 0.824999988079071  \n",
      "Validation Accuracy - 0.6279999017715454  Epoch 37, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5917051434516907, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6275998950004578  Epoch 37, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5347395539283752, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6271998882293701  Epoch 38, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.7030317783355713, Training Accuracy - 0.800000011920929  \n",
      "Validation Accuracy - 0.6275998950004578  Epoch 38, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.5257656574249268, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6267998814582825  Epoch 38, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4880875051021576, Training Accuracy - 0.824999988079071  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 38, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5641413927078247, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6273998618125916  Epoch 38, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.5142933130264282, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6311998963356018  Epoch 39, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.704436719417572, Training Accuracy - 0.7750000357627869  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 39, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.505355179309845, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6279999613761902  Epoch 39, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.45972442626953125, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6277998685836792  Epoch 39, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.534457802772522, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6367998719215393  Epoch 39, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.48187726736068726, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6289998888969421  Epoch 40, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6785627603530884, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6297998428344727  Epoch 40, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.4802810549736023, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6293998956680298  Epoch 40, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4680778980255127, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6291999220848083  Epoch 40, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5335764288902283, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 40, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.483961820602417, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6341999769210815  Epoch 41, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6943449974060059, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6341999173164368  Epoch 41, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.4723604917526245, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 41, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.46674618124961853, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6405999064445496  Epoch 41, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5323194265365601, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6375998854637146  Epoch 41, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.47526198625564575, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6355998516082764  Epoch 42, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6643595695495605, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6317998766899109  Epoch 42, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.470480740070343, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6317998766899109  Epoch 42, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4449620246887207, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6359999179840088  Epoch 42, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.5144384503364563, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6307999491691589  Epoch 42, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.4658522605895996, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 43, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6448721885681152, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6337998509407043  Epoch 43, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.47768622636795044, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6317998766899109  Epoch 43, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4473702609539032, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6269999146461487  Epoch 43, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4975159764289856, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 43, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.4486371874809265, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6361998915672302  Epoch 44, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6254139542579651, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 44, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.43991342186927795, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6393998861312866  Epoch 44, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.43255168199539185, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 44, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.49138450622558594, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6383999586105347  Epoch 44, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.45784175395965576, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 45, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.6011820435523987, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 45, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.4572630524635315, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6349998712539673  Epoch 45, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.43720024824142456, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6363999247550964  Epoch 45, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4920710027217865, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6407999396324158  Epoch 45, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.41394251585006714, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6441999077796936  Epoch 46, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.589401364326477, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6399998664855957  Epoch 46, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.4440956115722656, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 46, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4287055432796478, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.637199878692627  Epoch 46, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.47776007652282715, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6417999267578125  Epoch 46, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.42435044050216675, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6439998745918274  Epoch 47, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5660239458084106, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 47, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.41672149300575256, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6419999599456787  Epoch 47, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.41965097188949585, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 47, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.47596973180770874, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 47, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.40363073348999023, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6419999003410339  Epoch 48, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5575646162033081, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6411998867988586  Epoch 48, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.39749082922935486, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6411998867988586  Epoch 48, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4062679409980774, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6347998976707458  Epoch 48, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4536314308643341, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6427999138832092  Epoch 48, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.4078332185745239, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.642599880695343  Epoch 49, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5698366761207581, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6417998671531677  Epoch 49, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.4032197594642639, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.637799859046936  Epoch 49, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.4171428680419922, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6363998651504517  Epoch 49, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.45819568634033203, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6461999416351318  Epoch 49, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.38672947883605957, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6449999213218689  Epoch 50, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5778708457946777, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6481999158859253  Epoch 50, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.39238449931144714, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6365998983383179  Epoch 50, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.41649338603019714, Training Accuracy - 0.8499999642372131  \n",
      "Validation Accuracy - 0.6417999267578125  Epoch 50, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4338153004646301, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6441998481750488  Epoch 50, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3585163652896881, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6439998745918274  Epoch 51, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5320865511894226, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6445999145507812  Epoch 51, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.36263328790664673, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6455998420715332  Epoch 51, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.41788679361343384, Training Accuracy - 0.8499999642372131  \n",
      "Validation Accuracy - 0.6405999064445496  Epoch 51, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4434223771095276, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6461998820304871  Epoch 51, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.36410069465637207, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6397998929023743  Epoch 52, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5553359389305115, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 52, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.37137168645858765, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6425999402999878  Epoch 52, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.38911399245262146, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6389998197555542  Epoch 52, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.44330495595932007, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6443998217582703  Epoch 52, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3826709985733032, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 53, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5350526571273804, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6541998386383057  Epoch 53, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.35162681341171265, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6441998481750488  Epoch 53, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3819535970687866, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6417999267578125  Epoch 53, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.41933345794677734, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 53, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3521530032157898, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6463998556137085  Epoch 54, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5234807729721069, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.643799901008606  Epoch 54, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.3510705828666687, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 54, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3881877362728119, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.643799901008606  Epoch 54, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4079824686050415, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6471999287605286  Epoch 54, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.35400861501693726, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6405999064445496  Epoch 55, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5272547006607056, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6465998888015747  Epoch 55, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.3463192880153656, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 55, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.39625129103660583, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6337999105453491  Epoch 55, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.4009345471858978, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 55, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.34912723302841187, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.640999972820282  Epoch 56, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5118391513824463, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6531998515129089  Epoch 56, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.32087433338165283, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6423998475074768  Epoch 56, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.35546553134918213, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 56, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3752029836177826, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.650999903678894  Epoch 56, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.34900498390197754, Training Accuracy - 0.9249999523162842  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 57, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5046225786209106, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6515998840332031  Epoch 57, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.3555452823638916, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6475999355316162  Epoch 57, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3768177032470703, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6353998780250549  Epoch 57, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.386014461517334, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6527998447418213  Epoch 57, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.36848974227905273, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 58, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.5051623582839966, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.646399974822998  Epoch 58, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.3184167146682739, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6403999328613281  Epoch 58, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.34388983249664307, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6417998671531677  Epoch 58, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3722829222679138, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6497998237609863  Epoch 58, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3154609501361847, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6451998949050903  Epoch 59, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4762069284915924, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6513998508453369  Epoch 59, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.31949615478515625, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6433998346328735  Epoch 59, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3872798681259155, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6325998902320862  Epoch 59, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.37908729910850525, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.652199923992157  Epoch 59, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3301653563976288, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6427998542785645  Epoch 60, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4702144265174866, Training Accuracy - 0.8500000238418579  \n",
      "Validation Accuracy - 0.6449999213218689  Epoch 60, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.28402310609817505, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6477999091148376  Epoch 60, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3504682183265686, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 60, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3377106189727783, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6493998765945435  Epoch 60, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.32721319794654846, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6427998542785645  Epoch 61, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4755651354789734, Training Accuracy - 0.8250000476837158  \n",
      "Validation Accuracy - 0.6477998495101929  Epoch 61, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.3065260946750641, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6495998501777649  Epoch 61, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.34991106390953064, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6391999125480652  Epoch 61, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3296855092048645, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6541998982429504  Epoch 61, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.31504392623901367, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6389999389648438  Epoch 62, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4338879883289337, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 62, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2957139015197754, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6431999206542969  Epoch 62, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.34757000207901, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6405999064445496  Epoch 62, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.31279709935188293, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6453998684883118  Epoch 62, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.3040097951889038, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 63, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.443338006734848, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6453998684883118  Epoch 63, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.28677549958229065, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6451998949050903  Epoch 63, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.36153489351272583, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6377999186515808  Epoch 63, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.32953357696533203, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6499998569488525  Epoch 63, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.288137286901474, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6447999477386475  Epoch 64, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4549104869365692, Training Accuracy - 0.8750000596046448  \n",
      "Validation Accuracy - 0.6399998664855957  Epoch 64, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.284435898065567, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.64739990234375  Epoch 64, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.33752769231796265, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6369999647140503  Epoch 64, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.32327234745025635, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6479998826980591  Epoch 64, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.28608494997024536, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6453998684883118  Epoch 65, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.43864551186561584, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6451998949050903  Epoch 65, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.27241238951683044, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6461998820304871  Epoch 65, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3277871608734131, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.637799859046936  Epoch 65, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3046724200248718, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6513998508453369  Epoch 65, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2798319458961487, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6423999071121216  Epoch 66, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.43957456946372986, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6467999219894409  Epoch 66, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2781795859336853, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.644399881362915  Epoch 66, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.32220134139060974, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6443999409675598  Epoch 66, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.3091110587120056, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6521998643875122  Epoch 66, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2856636047363281, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 67, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4009329080581665, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6515998840332031  Epoch 67, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2685321867465973, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.645599901676178  Epoch 67, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3214268684387207, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6423999071121216  Epoch 67, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.293204128742218, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6467999219894409  Epoch 67, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.27272284030914307, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6469998955726624  Epoch 68, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.40720120072364807, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6493998765945435  Epoch 68, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.23863519728183746, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6489998698234558  Epoch 68, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3119775056838989, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6367999315261841  Epoch 68, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.29872435331344604, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6495999097824097  Epoch 68, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2719336152076721, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6445998549461365  Epoch 69, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.4030686318874359, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6433999538421631  Epoch 69, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2409777045249939, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6495999097824097  Epoch 69, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.31155699491500854, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 69, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2945631742477417, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.650999903678894  Epoch 69, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.272400438785553, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6439998745918274  Epoch 70, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.38405439257621765, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6441999077796936  Epoch 70, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.22439835965633392, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.650399923324585  Epoch 70, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3195296823978424, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 70, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2722487449645996, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6505998373031616  Epoch 70, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.26904112100601196, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6389998197555542  Epoch 71, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.40325191617012024, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6493998169898987  Epoch 71, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.23963883519172668, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6457998752593994  Epoch 71, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.3159544765949249, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6397998929023743  Epoch 71, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.26408326625823975, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6539998650550842  Epoch 71, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2674560546875, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6409998536109924  Epoch 72, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3527262210845947, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6421999335289001  Epoch 72, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.23202668130397797, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.645599901676178  Epoch 72, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.30813175439834595, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6431998610496521  Epoch 72, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2755977511405945, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6495998501777649  Epoch 72, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2507520914077759, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6417998671531677  Epoch 73, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3536982536315918, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6467998623847961  Epoch 73, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.24532295763492584, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6419999003410339  Epoch 73, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.28437942266464233, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6415998935699463  Epoch 73, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.260747492313385, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6525998711585999  Epoch 73, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2581126391887665, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6399998664855957  Epoch 74, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.372844934463501, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6445999145507812  Epoch 74, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2229537069797516, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.643799901008606  Epoch 74, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2955273985862732, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6433998942375183  Epoch 74, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.25334417819976807, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6513999104499817  Epoch 74, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2451993227005005, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6415998935699463  Epoch 75, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.34546995162963867, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6471998691558838  Epoch 75, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.22611993551254272, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6477999091148376  Epoch 75, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2785899341106415, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 75, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2442273199558258, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6505998969078064  Epoch 75, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2490392029285431, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6381998658180237  Epoch 76, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.33718740940093994, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6435999274253845  Epoch 76, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.22117017209529877, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6469998955726624  Epoch 76, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.27412375807762146, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6431999206542969  Epoch 76, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.23505833745002747, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6537998914718628  Epoch 76, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.23952345550060272, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6433998942375183  Epoch 77, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3382681906223297, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6411999464035034  Epoch 77, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.19403010606765747, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6457999348640442  Epoch 77, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2755589187145233, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 77, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2597511410713196, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6459999084472656  Epoch 77, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.24081872403621674, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 78, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3451113998889923, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6417998671531677  Epoch 78, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.21531721949577332, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6487998962402344  Epoch 78, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.27100786566734314, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6469998955726624  Epoch 78, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2561813294887543, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6473998427391052  Epoch 78, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.237142875790596, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 79, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3242364227771759, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 79, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.20416627824306488, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6519998908042908  Epoch 79, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2737762928009033, Training Accuracy - 0.875  \n",
      "Validation Accuracy - 0.6375998854637146  Epoch 79, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.24910956621170044, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6523998975753784  Epoch 79, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.22635141015052795, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6415998935699463  Epoch 80, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.32888293266296387, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6419999003410339  Epoch 80, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.19660457968711853, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6469998955726624  Epoch 80, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.25041642785072327, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6409999132156372  Epoch 80, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.24786877632141113, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6477998495101929  Epoch 80, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.2276885211467743, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6369999051094055  Epoch 81, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3465953469276428, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6435999274253845  Epoch 81, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18331405520439148, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6507998704910278  Epoch 81, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.25612929463386536, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6323999166488647  Epoch 81, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.23540738224983215, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6493998765945435  Epoch 81, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.21637758612632751, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6495999097824097  Epoch 82, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.3091849684715271, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.643799901008606  Epoch 82, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.2102316915988922, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6455998420715332  Epoch 82, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.26568546891212463, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.634199857711792  Epoch 82, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2187321037054062, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6489998698234558  Epoch 82, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.21296793222427368, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6415998339653015  Epoch 83, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.301262766122818, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6439998149871826  Epoch 83, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18484874069690704, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6455998420715332  Epoch 83, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.24166469275951385, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6423998475074768  Epoch 83, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.24062085151672363, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6471999287605286  Epoch 83, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.19651758670806885, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6459998488426208  Epoch 84, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2914198637008667, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6433998942375183  Epoch 84, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18935488164424896, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6453998684883118  Epoch 84, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.24381059408187866, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6445999145507812  Epoch 84, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.23668178915977478, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 84, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.22225430607795715, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 85, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2619754672050476, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6461998820304871  Epoch 85, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.19173917174339294, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6451998949050903  Epoch 85, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.22931735217571259, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6451999545097351  Epoch 85, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.21719497442245483, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6459998488426208  Epoch 85, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.20875824987888336, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6403999328613281  Epoch 86, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.26840850710868835, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6441998481750488  Epoch 86, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18881188333034515, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6469998955726624  Epoch 86, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.23484349250793457, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 86, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.23769862949848175, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.64739990234375  Epoch 86, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.202422097325325, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6349998712539673  Epoch 87, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.26516732573509216, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 87, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.16654713451862335, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6507998704910278  Epoch 87, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2367183268070221, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 87, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.21268676221370697, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6423999071121216  Epoch 87, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1931195855140686, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 88, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.27894383668899536, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 88, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.19451093673706055, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6463999152183533  Epoch 88, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.23168201744556427, Training Accuracy - 0.8999999761581421  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 88, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.21367588639259338, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6465998888015747  Epoch 88, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.19320861995220184, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 89, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.25203436613082886, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 89, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.17070429027080536, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.64739990234375  Epoch 89, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.22204875946044922, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 89, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2173980176448822, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6441999077796936  Epoch 89, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.21422171592712402, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.637199878692627  Epoch 90, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2315552979707718, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.64739990234375  Epoch 90, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18133270740509033, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6477998495101929  Epoch 90, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.22610238194465637, Training Accuracy - 0.9000000357627869  \n",
      "Validation Accuracy - 0.6285998821258545  Epoch 90, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.19261860847473145, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6475998163223267  Epoch 90, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.180841863155365, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6373999118804932  Epoch 91, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.24008449912071228, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 91, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1653144657611847, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.648399829864502  Epoch 91, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.21328461170196533, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6409998536109924  Epoch 91, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.20211178064346313, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6415998935699463  Epoch 91, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.19647099077701569, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 92, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.25437530875205994, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6367999315261841  Epoch 92, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1540294587612152, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.639799952507019  Epoch 92, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.20840410888195038, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 92, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.20019683241844177, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6505998373031616  Epoch 92, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1760326623916626, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 93, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2344360500574112, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 93, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.18051843345165253, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.644399881362915  Epoch 93, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.20811206102371216, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6361998915672302  Epoch 93, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2087564319372177, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 93, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.17472250759601593, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6413998603820801  Epoch 94, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.24021078646183014, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6411998867988586  Epoch 94, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.16167207062244415, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6453998684883118  Epoch 94, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2033558338880539, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6385999321937561  Epoch 94, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.20289510488510132, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6435998678207397  Epoch 94, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.16499927639961243, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391999125480652  Epoch 95, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.22359788417816162, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6447999477386475  Epoch 95, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.16030725836753845, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 95, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.20681336522102356, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6361998319625854  Epoch 95, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2006654292345047, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.641799807548523  Epoch 95, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.18549031019210815, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 96, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.21556681394577026, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6377999186515808  Epoch 96, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.16287526488304138, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6467999219894409  Epoch 96, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.2200077325105667, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 96, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2055286318063736, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 96, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.17240554094314575, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6405999064445496  Epoch 97, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.21575315296649933, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 97, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.13377831876277924, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6431998610496521  Epoch 97, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.21067561209201813, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6397998332977295  Epoch 97, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.198815256357193, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6451999545097351  Epoch 97, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.17928333580493927, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 98, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.22374242544174194, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 98, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.14409759640693665, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 98, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.19243589043617249, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6465999484062195  Epoch 98, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.17782223224639893, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6441999077796936  Epoch 98, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.16133099794387817, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391998529434204  Epoch 99, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2181919664144516, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.643799901008606  Epoch 99, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1524488776922226, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6479998826980591  Epoch 99, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1923733651638031, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 99, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.2009865939617157, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383998394012451  Epoch 99, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1616078019142151, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6401998996734619  Epoch 100, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.21597538888454437, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6395998597145081  Epoch 100, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.15371838212013245, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6431998610496521  Epoch 100, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.19537439942359924, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 100, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1932336390018463, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6381998658180237  Epoch 100, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1762811839580536, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6331998705863953  Epoch 101, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19512423872947693, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6407999396324158  Epoch 101, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.15181654691696167, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6385999321937561  Epoch 101, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1826455444097519, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6353998780250549  Epoch 101, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.18813985586166382, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6413998007774353  Epoch 101, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1647224724292755, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6407999396324158  Epoch 102, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.2106318175792694, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 102, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.15120330452919006, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6403998732566833  Epoch 102, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.19112266600131989, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.637199878692627  Epoch 102, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.20262910425662994, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6385999321937561  Epoch 102, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.14190258085727692, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 103, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20680153369903564, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6423999071121216  Epoch 103, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.13837508857250214, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6461999416351318  Epoch 103, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1788346916437149, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6339998841285706  Epoch 103, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.19565027952194214, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6413998603820801  Epoch 103, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.16182219982147217, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391998529434204  Epoch 104, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20491398870944977, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6381998658180237  Epoch 104, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1440381407737732, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6381999254226685  Epoch 104, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1888609528541565, Training Accuracy - 0.925000011920929  \n",
      "Validation Accuracy - 0.6303998827934265  Epoch 104, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.14775648713111877, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6431998610496521  Epoch 104, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.16176839172840118, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383998394012451  Epoch 105, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19724789261817932, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391998529434204  Epoch 105, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.14597713947296143, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6457998752593994  Epoch 105, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.19092601537704468, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 105, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.18328061699867249, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6421998739242554  Epoch 105, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.13450387120246887, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 106, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.20871701836585999, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6387998461723328  Epoch 106, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1261155605316162, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 106, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.17322181165218353, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 106, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.15262123942375183, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6429998874664307  Epoch 106, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.14199121296405792, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 107, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19201108813285828, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.637199878692627  Epoch 107, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.14797770977020264, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6415998935699463  Epoch 107, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.17818446457386017, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 107, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1612403839826584, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6369998455047607  Epoch 107, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.13777831196784973, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391999125480652  Epoch 108, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.18514005839824677, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6373999118804932  Epoch 108, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.13296347856521606, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6391998529434204  Epoch 108, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.17751803994178772, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 108, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.16845746338367462, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6419999003410339  Epoch 108, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.15327651798725128, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 109, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.18699073791503906, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6379998922348022  Epoch 109, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.12290319055318832, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6409998536109924  Epoch 109, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.17693446576595306, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6355999112129211  Epoch 109, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.18068018555641174, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 109, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.14368408918380737, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383999586105347  Epoch 110, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1740035116672516, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 110, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1240592896938324, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6393998861312866  Epoch 110, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1677739918231964, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 110, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.16413158178329468, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375998258590698  Epoch 110, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.15040552616119385, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6389999389648438  Epoch 111, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1874474138021469, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6329998970031738  Epoch 111, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11782936006784439, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6417999267578125  Epoch 111, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1673279106616974, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 111, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1523638665676117, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6331998705863953  Epoch 111, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1421683430671692, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6367998719215393  Epoch 112, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.17657625675201416, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6355999708175659  Epoch 112, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.12323145568370819, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.637199878692627  Epoch 112, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15928025543689728, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6363999247550964  Epoch 112, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1587657630443573, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6363998651504517  Epoch 112, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.14252671599388123, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6379998922348022  Epoch 113, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.19875362515449524, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999755859375  Epoch 113, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11656340211629868, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6385999917984009  Epoch 113, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1603800356388092, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6339998841285706  Epoch 113, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1504148542881012, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6345999240875244  Epoch 113, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.1481086015701294, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6329998970031738  Epoch 114, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1721162647008896, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.637799859046936  Epoch 114, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.13016754388809204, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6385998725891113  Epoch 114, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.16148775815963745, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6423999071121216  Epoch 114, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.14225560426712036, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.638999879360199  Epoch 114, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.13841697573661804, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 115, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.16471043229103088, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375998854637146  Epoch 115, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11500007659196854, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6395999193191528  Epoch 115, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15540850162506104, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6331998705863953  Epoch 115, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.13763253390789032, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6389999389648438  Epoch 115, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.13085347414016724, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6409999132156372  Epoch 116, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.16582848131656647, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357999444007874  Epoch 116, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1248352974653244, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6355998516082764  Epoch 116, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.16169585287570953, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6329998970031738  Epoch 116, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.15339809656143188, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 116, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.138053297996521, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 117, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1720309555530548, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999633789062  Epoch 117, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11553031206130981, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6339998841285706  Epoch 117, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15366825461387634, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6353998184204102  Epoch 117, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.14859792590141296, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315998435020447  Epoch 117, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.12644487619400024, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 118, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.16071242094039917, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6361998319625854  Epoch 118, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11076223105192184, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6413998603820801  Epoch 118, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15494485199451447, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6319999098777771  Epoch 118, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1372787058353424, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6365998983383179  Epoch 118, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.12120121717453003, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6399999260902405  Epoch 119, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1656591296195984, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.636199951171875  Epoch 119, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1204250305891037, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 119, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15236598253250122, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6309998631477356  Epoch 119, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1339237093925476, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6353999376296997  Epoch 119, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.12543049454689026, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6387999057769775  Epoch 120, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1654340922832489, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6311998963356018  Epoch 120, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.10918674618005753, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 120, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.150821715593338, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6349999308586121  Epoch 120, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.13930979371070862, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.638999879360199  Epoch 120, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10537424683570862, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6369998455047607  Epoch 121, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.16303251683712006, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.634399950504303  Epoch 121, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11948803067207336, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999627113342  Epoch 121, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.15422435104846954, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6325998902320862  Epoch 121, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1551426351070404, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6339998841285706  Epoch 121, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.12444594502449036, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6363998651504517  Epoch 122, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.15713627636432648, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 122, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11409568786621094, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 122, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13758674263954163, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 122, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1576465368270874, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 122, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.11490204930305481, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 123, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.17166797816753387, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6347998380661011  Epoch 123, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08750581741333008, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6329998970031738  Epoch 123, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13972201943397522, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6297999024391174  Epoch 123, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.13710811734199524, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 123, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10741865634918213, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6393998861312866  Epoch 124, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.148079514503479, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6361998915672302  Epoch 124, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.10926829278469086, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 124, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1407918483018875, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6307998895645142  Epoch 124, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.13559067249298096, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6363998651504517  Epoch 124, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10350385308265686, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 125, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13886314630508423, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327998638153076  Epoch 125, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1099603921175003, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 125, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13981832563877106, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6301999092102051  Epoch 125, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.14103282988071442, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 125, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10450221598148346, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6351998448371887  Epoch 126, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1480814516544342, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6351999044418335  Epoch 126, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11204960197210312, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6241999268531799  Epoch 126, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13811391592025757, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6319998502731323  Epoch 126, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.12887011468410492, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6311999559402466  Epoch 126, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10030104219913483, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6345999240875244  Epoch 127, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14366641640663147, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6369998455047607  Epoch 127, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11248436570167542, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6277998685836792  Epoch 127, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13034655153751373, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 127, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.12378454953432083, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6345999240875244  Epoch 127, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09738771617412567, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 128, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1304948776960373, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 128, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1126023679971695, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6283999085426331  Epoch 128, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.150656059384346, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6341999173164368  Epoch 128, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.13512885570526123, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999159812927  Epoch 128, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10440056025981903, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6319998502731323  Epoch 129, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14163459837436676, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6377999186515808  Epoch 129, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.1189948171377182, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6181999444961548  Epoch 129, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13161343336105347, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6263999342918396  Epoch 129, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1384260356426239, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6301999092102051  Epoch 129, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10341624915599823, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6355999708175659  Epoch 130, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.145330011844635, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6367998719215393  Epoch 130, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.10643105953931808, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6225998997688293  Epoch 130, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1386767476797104, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6293999552726746  Epoch 130, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.12066639959812164, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6297999024391174  Epoch 130, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10780461132526398, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6397998929023743  Epoch 131, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14409399032592773, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357998847961426  Epoch 131, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.11683283746242523, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 131, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1528863161802292, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6353999376296997  Epoch 131, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.1160266101360321, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6271998882293701  Epoch 131, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10979447513818741, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327998638153076  Epoch 132, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1400981843471527, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6359999179840088  Epoch 132, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.09964925050735474, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6279999017715454  Epoch 132, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13833625614643097, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.640799880027771  Epoch 132, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.11616654694080353, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999159812927  Epoch 132, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10080341249704361, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999633789062  Epoch 133, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.14286699891090393, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6381998658180237  Epoch 133, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08960402756929398, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 133, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13528841733932495, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6399998664855957  Epoch 133, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.11117095500230789, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6295998692512512  Epoch 133, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09851202368736267, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6367998719215393  Epoch 134, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1268317550420761, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6369998455047607  Epoch 134, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.10512632131576538, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6189998984336853  Epoch 134, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13938777148723602, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6383998990058899  Epoch 134, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.11859569698572159, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6303998231887817  Epoch 134, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.10749490559101105, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375999450683594  Epoch 135, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12531894445419312, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6311998963356018  Epoch 135, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0939805880188942, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6169999241828918  Epoch 135, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12545005977153778, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6355998516082764  Epoch 135, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10561444610357285, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6257998943328857  Epoch 135, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08819195628166199, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6365998983383179  Epoch 136, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13143333792686462, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357999444007874  Epoch 136, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.09768079221248627, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6251999139785767  Epoch 136, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.14342351257801056, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.626599907875061  Epoch 136, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10848227888345718, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6227999329566956  Epoch 136, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.096234031021595, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 137, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12723666429519653, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 137, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08292340487241745, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.624599814414978  Epoch 137, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1432378739118576, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6267999410629272  Epoch 137, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.11811098456382751, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6279999017715454  Epoch 137, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08685861527919769, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6375998854637146  Epoch 138, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13910502195358276, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6353999376296997  Epoch 138, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08788304030895233, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6253998875617981  Epoch 138, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13340598344802856, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 138, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10392637550830841, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6307998895645142  Epoch 138, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08485692739486694, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6341999173164368  Epoch 139, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1343788504600525, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6321998834609985  Epoch 139, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08152281492948532, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6195998787879944  Epoch 139, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12418362498283386, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6321998834609985  Epoch 139, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10754675418138504, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999159812927  Epoch 139, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09017867594957352, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999633789062  Epoch 140, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.13465510308742523, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6337999105453491  Epoch 140, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08676617592573166, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6223998665809631  Epoch 140, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1222103089094162, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 140, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10277239978313446, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 140, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08920176327228546, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 141, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11787915229797363, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 141, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08175919950008392, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.616399884223938  Epoch 141, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13001254200935364, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6273998618125916  Epoch 141, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10655541718006134, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6297999620437622  Epoch 141, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08900480717420578, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 142, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11425229907035828, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999159812927  Epoch 142, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.09342479705810547, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6137999296188354  Epoch 142, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13214503228664398, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6323999166488647  Epoch 142, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08928782492876053, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999627113342  Epoch 142, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09781412780284882, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 143, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12287993729114532, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6309998631477356  Epoch 143, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07956922054290771, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6221998929977417  Epoch 143, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12280446290969849, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6279999017715454  Epoch 143, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10200139880180359, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6311999559402466  Epoch 143, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08438880741596222, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6299998760223389  Epoch 144, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11512276530265808, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 144, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07446712255477905, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219998598098755  Epoch 144, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13278712332248688, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6353998780250549  Epoch 144, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.10015292465686798, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6239999532699585  Epoch 144, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08903521299362183, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6309999227523804  Epoch 145, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11898631602525711, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6315999031066895  Epoch 145, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0866268053650856, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6177999377250671  Epoch 145, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12276697158813477, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6369999051094055  Epoch 145, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08899036049842834, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.627799928188324  Epoch 145, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09439198672771454, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 146, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11439935863018036, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6363999247550964  Epoch 146, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07143937796354294, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6269999146461487  Epoch 146, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12882108986377716, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6333998441696167  Epoch 146, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09413926303386688, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6281998753547668  Epoch 146, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07918864488601685, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 147, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11194264888763428, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6329998970031738  Epoch 147, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0945507362484932, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6189998984336853  Epoch 147, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11520644277334213, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6345998048782349  Epoch 147, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09431396424770355, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6251998543739319  Epoch 147, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.09178996086120605, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 148, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11208247393369675, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6359999179840088  Epoch 148, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08177574723958969, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6203998923301697  Epoch 148, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1297762095928192, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.634199857711792  Epoch 148, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08215584605932236, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6263998746871948  Epoch 148, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08960462361574173, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 149, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10498254001140594, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6319999694824219  Epoch 149, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08092456310987473, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6201998591423035  Epoch 149, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12696698307991028, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6325998902320862  Epoch 149, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09355837851762772, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6263999342918396  Epoch 149, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08548789471387863, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6319999098777771  Epoch 150, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12409204244613647, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6281998753547668  Epoch 150, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08563274145126343, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.614799976348877  Epoch 150, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12010978907346725, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6297999024391174  Epoch 150, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09714583307504654, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6261999607086182  Epoch 150, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08635269850492477, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.621799886226654  Epoch 151, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10999339073896408, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6347998976707458  Epoch 151, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0849582701921463, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6281998753547668  Epoch 151, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12278363853693008, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 151, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09570546448230743, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6241998672485352  Epoch 151, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08278662711381912, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6317999362945557  Epoch 152, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10965859889984131, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6355999112129211  Epoch 152, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0701933428645134, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.625999927520752  Epoch 152, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12179639935493469, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6289999485015869  Epoch 152, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09353120625019073, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 152, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08446522057056427, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6187999248504639  Epoch 153, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1135256439447403, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.629599928855896  Epoch 153, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06472282111644745, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6247999668121338  Epoch 153, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12026844173669815, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6257998943328857  Epoch 153, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09120005369186401, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6221999526023865  Epoch 153, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08164528012275696, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6311998963356018  Epoch 154, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10793636739253998, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327999234199524  Epoch 154, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0871693566441536, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6177998185157776  Epoch 154, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13004308938980103, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6333999037742615  Epoch 154, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.09200285375118256, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6235998868942261  Epoch 154, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07478570938110352, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6305999159812927  Epoch 155, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10399173945188522, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6357998251914978  Epoch 155, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07952416688203812, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.61819988489151  Epoch 155, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.13672097027301788, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6353998780250549  Epoch 155, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08546256273984909, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6235998868942261  Epoch 155, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08031956851482391, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6317999362945557  Epoch 156, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.12756401300430298, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 156, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.08241963386535645, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6197998523712158  Epoch 156, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12731781601905823, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6249998807907104  Epoch 156, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07842349261045456, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6237999796867371  Epoch 156, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06763613224029541, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6287997961044312  Epoch 157, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10571884363889694, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6281999349594116  Epoch 157, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07749338448047638, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 157, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12063644826412201, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6347999572753906  Epoch 157, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08524090051651001, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6245998740196228  Epoch 157, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.086483433842659, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6239998936653137  Epoch 158, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10220838338136673, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6301998496055603  Epoch 158, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06902458518743515, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 158, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11329106986522675, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6381998658180237  Epoch 158, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07703551650047302, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6269998550415039  Epoch 158, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08152632415294647, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6167998909950256  Epoch 159, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11688894033432007, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6269999146461487  Epoch 159, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07049544155597687, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6209998726844788  Epoch 159, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1183474212884903, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6343998908996582  Epoch 159, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07918386161327362, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.622999906539917  Epoch 159, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.0766308456659317, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6271998882293701  Epoch 160, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10442700982093811, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6355998516082764  Epoch 160, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07109115272760391, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6169999241828918  Epoch 160, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11539018899202347, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6263999342918396  Epoch 160, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07175632566213608, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6279999017715454  Epoch 160, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08674141764640808, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.616399884223938  Epoch 161, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09462898969650269, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6335998773574829  Epoch 161, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07126456499099731, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6187999248504639  Epoch 161, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1156286671757698, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6309999227523804  Epoch 161, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08072906732559204, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6249998807907104  Epoch 161, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08140286058187485, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6271999478340149  Epoch 162, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09962448477745056, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6271998882293701  Epoch 162, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06860262900590897, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6191999316215515  Epoch 162, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12567855417728424, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6311998963356018  Epoch 162, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07734602689743042, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 162, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07645344734191895, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.624799907207489  Epoch 163, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10313481837511063, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6291999220848083  Epoch 163, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.061790309846401215, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6231999397277832  Epoch 163, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11305952817201614, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 163, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08061622828245163, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6289999485015869  Epoch 163, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07917089760303497, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.625999927520752  Epoch 164, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.103999063372612, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6283999085426331  Epoch 164, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0663008987903595, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6195999383926392  Epoch 164, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10528409481048584, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6273998618125916  Epoch 164, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07418958842754364, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6327998638153076  Epoch 164, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07634910941123962, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6251999139785767  Epoch 165, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09181921929121017, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6231999397277832  Epoch 165, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0588790699839592, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6189998984336853  Epoch 165, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12820808589458466, Training Accuracy - 0.949999988079071  \n",
      "Validation Accuracy - 0.6313998699188232  Epoch 165, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06336282938718796, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6269999146461487  Epoch 165, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08986764401197433, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6185998916625977  Epoch 166, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08929574489593506, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.624799907207489  Epoch 166, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06870361417531967, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6239999532699585  Epoch 166, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.1083158403635025, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.637799859046936  Epoch 166, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06509246677160263, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.625999927520752  Epoch 166, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.0900268405675888, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6195999383926392  Epoch 167, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.11091342568397522, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 167, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07291985303163528, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6155998706817627  Epoch 167, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11492236703634262, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6261999011039734  Epoch 167, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06865764409303665, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6299998760223389  Epoch 167, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.08732680976390839, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6159999370574951  Epoch 168, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09150100499391556, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6255998611450195  Epoch 168, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.069823257625103, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6113998889923096  Epoch 168, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10352026671171188, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6291999220848083  Epoch 168, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07400096952915192, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6235998868942261  Epoch 168, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.062165938317775726, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6253998875617981  Epoch 169, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08786635100841522, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 169, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07426994293928146, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.612799882888794  Epoch 169, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10429630428552628, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6281999349594116  Epoch 169, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07234134525060654, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6243999004364014  Epoch 169, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07808324694633484, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6167999505996704  Epoch 170, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09285503625869751, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6253998875617981  Epoch 170, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07606007158756256, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6139999032020569  Epoch 170, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11020328104496002, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 170, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.060370802879333496, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6247998476028442  Epoch 170, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07534350454807281, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6155999302864075  Epoch 171, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.0876460075378418, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6227998733520508  Epoch 171, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06794948875904083, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6133999228477478  Epoch 171, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10794152319431305, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6293998956680298  Epoch 171, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06726713478565216, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6239998936653137  Epoch 171, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06537780165672302, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6257998943328857  Epoch 172, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10552964359521866, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6283999085426331  Epoch 172, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05754605308175087, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6165999174118042  Epoch 172, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10589294135570526, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6281998753547668  Epoch 172, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06613512337207794, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 172, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06856013834476471, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.619999885559082  Epoch 173, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.0936741977930069, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.61819988489151  Epoch 173, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06582090258598328, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.612799882888794  Epoch 173, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10606051236391068, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6299998760223389  Epoch 173, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06106939911842346, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6235998868942261  Epoch 173, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07483401894569397, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6217999458312988  Epoch 174, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10557690262794495, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6185998916625977  Epoch 174, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06632937490940094, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6141999363899231  Epoch 174, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09937767684459686, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 174, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.07524417340755463, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6279998421669006  Epoch 174, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07308946549892426, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6177999377250671  Epoch 175, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09481632709503174, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6249998807907104  Epoch 175, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06474155187606812, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6107999086380005  Epoch 175, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10808482021093369, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.631399929523468  Epoch 175, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.060549430549144745, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.622999906539917  Epoch 175, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06859026849269867, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6233999133110046  Epoch 176, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09639212489128113, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6257998943328857  Epoch 176, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07413125038146973, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.613399863243103  Epoch 176, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10750457644462585, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.63239985704422  Epoch 176, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06134460121393204, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6283998489379883  Epoch 176, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07431329786777496, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.61819988489151  Epoch 177, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.1092735305428505, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6217999458312988  Epoch 177, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06071924790740013, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6185999512672424  Epoch 177, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.0995333269238472, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 177, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06120074912905693, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6233999133110046  Epoch 177, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07298542559146881, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 178, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08849384635686874, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6209999322891235  Epoch 178, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06477957963943481, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6151999235153198  Epoch 178, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10009612888097763, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6309999227523804  Epoch 178, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.057203080505132675, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6263998746871948  Epoch 178, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07531215250492096, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6215999126434326  Epoch 179, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09466630220413208, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6231998801231384  Epoch 179, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0645759329199791, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.619399905204773  Epoch 179, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09850631654262543, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6263999342918396  Epoch 179, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05423145741224289, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6255998611450195  Epoch 179, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06555948406457901, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6249998807907104  Epoch 180, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09373633563518524, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6263998746871948  Epoch 180, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06958027184009552, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 180, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.12380211800336838, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6273999214172363  Epoch 180, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06821705400943756, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 180, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07193508744239807, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6233999133110046  Epoch 181, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.09145648032426834, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6223998665809631  Epoch 181, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07244639098644257, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6093999147415161  Epoch 181, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09977218508720398, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6277998685836792  Epoch 181, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05863046646118164, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6215999126434326  Epoch 181, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07873769849538803, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6181999444961548  Epoch 182, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10686870664358139, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6191998720169067  Epoch 182, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.07856719940900803, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.613599956035614  Epoch 182, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.103310227394104, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.626599907875061  Epoch 182, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05089087784290314, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 182, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.07304802536964417, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6185998916625977  Epoch 183, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07896709442138672, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6221998929977417  Epoch 183, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.049612149596214294, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6149998903274536  Epoch 183, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10933613777160645, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6289999485015869  Epoch 183, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06349959969520569, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6261999011039734  Epoch 183, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.058054469525814056, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.621799886226654  Epoch 184, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.0808897316455841, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6223999261856079  Epoch 184, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05105764791369438, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.619399905204773  Epoch 184, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.11148759722709656, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6245998740196228  Epoch 184, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.0515417754650116, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6255999207496643  Epoch 184, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06025390699505806, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6285998821258545  Epoch 185, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07070481032133102, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6157999038696289  Epoch 185, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05522799491882324, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6197998523712158  Epoch 185, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09942013025283813, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6241998672485352  Epoch 185, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.08055789768695831, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6221998929977417  Epoch 185, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06295344978570938, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.626599907875061  Epoch 186, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.0930975005030632, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6177998781204224  Epoch 186, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06651562452316284, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6167998909950256  Epoch 186, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10366535931825638, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6263998746871948  Epoch 186, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.050622954964637756, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 186, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.05836187303066254, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6225998997688293  Epoch 187, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10372145473957062, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6197999119758606  Epoch 187, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06365962326526642, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6169998645782471  Epoch 187, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09964963793754578, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6239998936653137  Epoch 187, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05573774874210358, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6245999336242676  Epoch 187, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.061331942677497864, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6213998794555664  Epoch 188, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.10300971567630768, Training Accuracy - 0.9750000238418579  \n",
      "Validation Accuracy - 0.6167999505996704  Epoch 188, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.053766340017318726, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.616399884223938  Epoch 188, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10702794790267944, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6267999410629272  Epoch 188, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05199052393436432, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.621799886226654  Epoch 188, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.058654554188251495, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6205998659133911  Epoch 189, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08565568923950195, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6177999377250671  Epoch 189, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05450192838907242, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6117998957633972  Epoch 189, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.08382393419742584, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6261999607086182  Epoch 189, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05019494891166687, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6261999011039734  Epoch 189, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.058976005762815475, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.621799886226654  Epoch 190, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07649396359920502, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6129999160766602  Epoch 190, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06018201261758804, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6117998957633972  Epoch 190, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10079054534435272, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6201999187469482  Epoch 190, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05905711650848389, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6223999261856079  Epoch 190, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.0570971742272377, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6227998733520508  Epoch 191, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07501325756311417, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6137998700141907  Epoch 191, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05608723312616348, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6185999512672424  Epoch 191, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.08993203938007355, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6235998868942261  Epoch 191, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.061908066272735596, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6207998991012573  Epoch 191, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06258922815322876, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6183999180793762  Epoch 192, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.06639225035905838, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.619399905204773  Epoch 192, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.06219002231955528, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6139999032020569  Epoch 192, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.08520911633968353, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6207999587059021  Epoch 192, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05957406014204025, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 192, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06799796968698502, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.619999885559082  Epoch 193, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08117906749248505, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6147999167442322  Epoch 193, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.0512680858373642, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.612799882888794  Epoch 193, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10526662319898605, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6233999133110046  Epoch 193, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05489038676023483, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.625999927520752  Epoch 193, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06626205891370773, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6225998997688293  Epoch 194, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07319389283657074, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6219999194145203  Epoch 194, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.04914567992091179, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.619999885559082  Epoch 194, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.08779172599315643, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6241998672485352  Epoch 194, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05328723415732384, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.624799907207489  Epoch 194, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.0575491227209568, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6249998807907104  Epoch 195, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07321135699748993, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6161999106407166  Epoch 195, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.048840153962373734, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6151999235153198  Epoch 195, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.08296161144971848, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6233999133110046  Epoch 195, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.055798567831516266, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6225998997688293  Epoch 195, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.05794109031558037, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6205999255180359  Epoch 196, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.0847826674580574, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6163998246192932  Epoch 196, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.057518988847732544, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6123999357223511  Epoch 196, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09190355986356735, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6225998401641846  Epoch 196, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05654139071702957, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6199999451637268  Epoch 196, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.049784209579229355, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6223998665809631  Epoch 197, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07255947589874268, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6117998957633972  Epoch 197, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05526578798890114, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6111998558044434  Epoch 197, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09166790544986725, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6203998923301697  Epoch 197, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06642801314592361, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6153998970985413  Epoch 197, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.06192848086357117, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6215999126434326  Epoch 198, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07416554540395737, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6089999675750732  Epoch 198, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.059635959565639496, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6137998700141907  Epoch 198, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.0937572792172432, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.621199905872345  Epoch 198, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.05059868469834328, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6227998733520508  Epoch 198, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.051120903342962265, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6235998272895813  Epoch 199, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.07089069485664368, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6141998767852783  Epoch 199, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.05271626263856888, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6121998429298401  Epoch 199, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.09697698801755905, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6121999621391296  Epoch 199, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.04835398495197296, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.627799928188324  Epoch 199, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.04877682402729988, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6221998929977417  Epoch 200, CIFAR-10 Batch 1:  \n",
      "Training Cost - 0.08032897114753723, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6195998787879944  Epoch 200, CIFAR-10 Batch 2:  \n",
      "Training Cost - 0.048987094312906265, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6139999032020569  Epoch 200, CIFAR-10 Batch 3:  \n",
      "Training Cost - 0.10087833553552628, Training Accuracy - 0.9749999642372131  \n",
      "Validation Accuracy - 0.6271998882293701  Epoch 200, CIFAR-10 Batch 4:  \n",
      "Training Cost - 0.06989926099777222, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6165999174118042  Epoch 200, CIFAR-10 Batch 5:  \n",
      "Training Cost - 0.055386193096637726, Training Accuracy - 1.0  \n",
      "Validation Accuracy - 0.6255998611450195  "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6277866242038217\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiISPT/AWYBznDekH1gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae6785d6a0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
